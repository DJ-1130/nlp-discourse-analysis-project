{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190b2863-71e4-4405-9e7e-149ed18bc556",
   "metadata": {},
   "source": [
    "전처리 및 LDA 토픽 모델링_코드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0382175-7f6b-42f5-8026-e84b5b50ac4c",
   "metadata": {},
   "source": [
    "본 코드는 크롤링한 뉴스 텍스트 데이터를 가지고 조사나 TF-IDF 점수를 통해 불필요한 불용어를 제거하여 정치적 올바름 키워드를 중심으로 한 뉴스 기사들의 단어빈도를 파악 후 , LDA 모델을 생성하는데 적정한 토픽 수를 정하였다. 토픽 수를 정하는데는 혼잡도와 복잡도를 시각화하여 확인해보았으나 유의미한 결과를 발견하지 못하여 pyLDAVis 시각화를 활용하여 토픽 별 독립성이 가장 높게 나타나는 토픽 수를 3을 선정하여 토픽모델링을 진행하였다. 토픽모델링 진행 결과, 토픽은 \"정치적 다양성과 공정성 담론\", \"전통 정치 담론과 정당 갈등\", \" 미국 보수 포퓰리즘과 정체성 정치의 충돌\" 으로 묶어볼 수 있었다. 토픽 순으로 2117, 1080, 934개의 뉴스 문서의 개수를 추출할 수 있었으며 토픽별 단어 포함 확률인 β와 문서의 토픽 분류 확률인 γ를 defalutdict로 추출하여 가장 관련된 문서들을 확인하였다. 연도별 토픽의 수를 확인하여 막대그래프로 시각화를 하였다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073371b9-5f82-405d-9850-936d1a0fe436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 1. CSV 불러오기\n",
    "merged_df = pd.read_csv('merged_df.csv', dtype=str)\n",
    "\n",
    "# 2. 제목과 본문 합치기\n",
    "merged_df['text'] = merged_df['제목'].fillna('') + ' ' + merged_df['본문'].fillna('')\n",
    "\n",
    "# 3. 영문자, 숫자, 특수기호 제거 (한글 및 공백만 남김)\n",
    "def clean_non_korean(text):\n",
    "    text = re.sub(r'[^가-힣\\s]', ' ', text)\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "merged_df['clean_text'] = merged_df['text'].apply(clean_non_korean)\n",
    "\n",
    "# 4. 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 5. 명사만 추출\n",
    "merged_df['nouns'] = merged_df['clean_text'].apply(lambda x: okt.nouns(x))\n",
    "\n",
    "# 6. 불용어 리스트 정의 (예시)\n",
    "domain_stopwords = set([\n",
    "    '뉴스', '기자', '기사', '보도', '언론', '인터뷰',\n",
    "    '발표', '전달', '보도자료', '관련', '오늘', '이번',\n",
    "    '좀','정치적', '수', '있다', '이', '대한', '있는', '한', '년', '등', '더',\n",
    "    '그', '것이다', '같은', '기자', '다른', '것은', '일', '전', '했다', '하는',\n",
    "    '미국', '이런', '위해', '하지만', '할', '지난', '월', '또', '것이', '대해',\n",
    "    '한다', '가장', '없는', '있다고', '것을', '그는', '및', '통해', '아니라',\n",
    "    '있다는', '때', '것', '많은', '중', '게', '최근', '모든', '큰', '어떤',\n",
    "    '무단' ,'배포' ,'전', '행사' ,'금지','정치적', '수', '있다', '이', '대한',\n",
    "    '있는', '한', '년', '등', '더', '그', '것이다', '같은', '기자', '다른', '것은', \n",
    "    '일', '전', '했다', '하는', '미국', '이런', '위해', '하지만', '할', '지난', '월',\n",
    "    '또', '것이', '대해', '한다', '없는', '가장', '있다고', '것을', '그는', '및', '통해',\n",
    "    '아니라', '있다는', '때', '것', '많은', '중', '게', '최근', '모든', '큰', '어떤',\n",
    "    '것으로', '이후', '함께', '때문에', '없다', '자신의', '될', '두', '그러나', '않는', '말했다', \n",
    "    '다시', '안', '우리', '특히', '하고', '된', '것이라고', '위한', '그런', '건', '아니다', '때문이다', '내', \n",
    "    '모두', '어떻게', '잘', '그리고', '이번', '그의', '지금', '왜', '되는', \n",
    "    '많이', '보면', '그런데', '대', '우리가', '이렇게', '그래서', '있습니다', '저는',\n",
    "    '는','다','역시','않은','나는','이제','여러','이를','내가','등을','당시','제가',\n",
    "    '의','않고','가','데','것도','한다고','있을','된다','물론','한다는','바로','사실',\n",
    "    '이미','에','그렇게','좋은','있고','말','사람','거','점','재','속','명','책','시작',\n",
    "    '힘','하나','교수','시간','문화','표현','이유','생각','문제','한국','이야기','대표','세계',\n",
    "    '후보','주장','상황','주의','작품','자신','정도','위','국가','모습','얘기','시대','뒤','못',\n",
    "    '가지','캐릭터','의미','경우','분','의원','발언','운동','결과','과정','그것','현실','작가','만','사건',\n",
    "    '평가','배우','를','등장','가치','경제','저','인간','걸','글','날','감독','입장','기업','존재','부분',\n",
    "    '무엇','앞','세상','자기','공개','주인공','개인','현재','강조','당','삶','계속','마음','방송','자체',\n",
    "    '방식','결정','인물','선택','과거','내용','분석','나라','개','사이','중심','일부','정당','때문','알','후',\n",
    "    '나','번','관객','년대','여기','사용','설명','처음','아이','뜻','선','미래','이름','지난해','보고','뿐','세',\n",
    "    '정의','적','시민','대상','자리','지역','볼','비','돈','온','곳','정치인','해','쪽','역할','사랑','권력','향',\n",
    "    '공격','관계','감','누구','달','대중','가능성','요구','라며','수도','올해','간','목소리','진행','정체','위기',\n",
    "    '개봉','표','이자','관심','살','오히려','층','질문','콘텐츠','정권','로','활동','시리즈','언어','행동','의견',\n",
    "    '온라인','장면','또한','진영','최고','달러','영향','세력','제','이슈','전략','이기','고민','첫','현상','듯'\n",
    "    ,'저자', '인기', '기존', '스스로', '주', '은', '집단', '바', '경험', '국제', '노력', '부정', \n",
    "    '기준', '국내', '동안', '여러분', '장관', '도널드', '줄', '학교', '로서', '논쟁', '요',\n",
    "    '기록', '관', '도', '눈', '기도', '반응','대신', '조사', '참여', '라면', '해결', '메시지', '차', \n",
    "    '거나', '비롯', '필요', '영상', '다음', '약', '시절', '제기', '서로', '의식', '선언',\n",
    "    '책임', '론', '단어', '포함', '직접', '편', '다만', '크게', '주목', '정말', \n",
    "    '지지율', '가운데', '몇', '우려', '김', '해당', '회사', '시', '언급', '사태', '출연', '독자',\n",
    "    '말씀', '인사', '전체', '인정', '더욱', '입', '공식', '태도', '기회', '저작권', \n",
    "    '답', '해도', '방법', '단체', '기술', '박', '상대', '사진', '조금', '제작', '당신', '해석', '핵심', '동시', '회', \n",
    "    '예', '거부', '제도', '일이', '집', '사례', '매우', '음악', '여론조사', '역', '연기', '팀','실제','이해','추구','정신',\n",
    "    '제대로','대부분','길','이미지','주요','리','앞서','거의','마지막','매체','확인','환경','운영','상태','그게','물','반면',\n",
    "     '정보', '논리', '수준', '세기', '거리', '면', '먼저',\n",
    "    '아버지', '공감', '뭐', '적극', '식', '배경', '친구', '지원',\n",
    "    '우선', '자', '이용', '워', '달리', '행위', '미국인', '만큼', '분위기',\n",
    "     '위원장', '위원회', '이전', '의도', '전혀', '투자', '란', '채',\n",
    "    '상징', '고', '음', '명의', '유지', '조', '감정', '초', '함',\n",
    "    '기', '누군가', '보기', '발생', '순간', '주류', '경찰', '앵커',\n",
    "    '방향', '개발', '인터넷', '조직', '최', \n",
    "    '스', '설정', '씨', '논의', '머리', '준비', '얼마나', '새', '성과',\n",
    "    '그대로', '시도', '인', '흥행', '끝', '손', '기반', '특정',\n",
    "    '즉', '마찬가지', '거기', '부', '원칙', '진', '위원', '몸', '반영', '해외',\n",
    "    '분노', '제목', '서비스', '전문가', '예술',\n",
    "    '과도', '본인', '흐름', '대화', '그녀', '할리우드', '묘사', '글로벌',\n",
    "    '주제', '요즘', '예상', '효과', '다수', '위협', '산업', '어디', '출마', '관점',\n",
    "    '소개', '프로그램', '성공', '집중', '대응', '정', '기억','주의자',\n",
    "    '과연', '구독', '수사', '한편', '장','불', '우리나라', '정치권', '일상', '시기', '제시',\n",
    "    '활용', '국회', '영역', '기간', '구성', '임', '비교', '토론', '외교', '노래','도전', '무대',\n",
    "    '기억', '커뮤니티', '피해', '당선인', '여론', '그동안', '사과',\n",
    "    '공간', '억', '문학', '총장', '당원', '디', '그냥', '개념', '연합뉴스',\n",
    "    '요소', '목표', '용어', '강', '계획', '지속', '단', '전망',\n",
    "    '스타', '불만', '측면', '네', '늘', '최초', '로부터', '웃음', '기본', '일단',\n",
    "    '절대', '티스', '맥락', '선수', '서사', '사상', '그때',\n",
    "    '광고', '이익', '확산', '교사', '지난달', '시선', '국정', '남', '희망',\n",
    "    '둘', '강요', '생활', '보', '차원', '추진','무슨','준','담론','사업','연구',\n",
    "    '발','문','매력','지사','발전','아주','출시','현장',\n",
    "    '현장', '일반', '피', '대사', '차지', '평론가', '크리스마스', '도시', '안보',\n",
    "    '얼굴', '총리', '막', '조롱', '스토리', '전환', '직원', '배',\n",
    "    '보이', '회의', '고려', '셈', '지층', '경선',\n",
    "    '각종', '장르', '누가', '재미', '도움', '월일', '기대', '채널', '분야'\n",
    "    '공연', '성장', '기관', '사고', '히어로', '최대', '대가', '불구', '터', '현', '쇼', '댓글', '시스템', '진짜', '반복', '무시',\n",
    "    '제공', '지도자', '전문', '스타워즈', '만원', '선정', '행보', '수가', \n",
    "    '동의', '며', '이용자', '충격', '원인', '차례', '그룹',\n",
    "    '신문', '주변', '원','제안','예정', '실사',\n",
    "    '내부', '부통령', '과학', '내년', '근거', '치', '탓', '목적',\n",
    "    '업계', '진실', '마련', '막말', '소재', '의회', '마치', '공유', '화제', '연설',\n",
    "    '작업','스포츠', '변호사', '난', '계기', '느낌', '소리', '심지어',\n",
    "    '형성', '만약', '정서', '관리', '중이', '한번', '판', '지구', '꿈','서구', '각','모델',\n",
    "    '반', '아래', '소비', '주민',  '합의', '처', '싸움',\n",
    "    '잘못', '창', '일종', '옹호', '상상', '포인트', '긍정','인생','원래','형태','공약','가수','주자','확장',\n",
    "    '부동산','걱정', '에리얼', '그림', '얼마', '가정', '주체', '국',\n",
    "    '바탕', '아무', '주로', '경향', '심','근본', '질', '수정', '탈', '항상',\n",
    "    '생', '보장', '전국', '인상', '달라','주지','플로리다주',\n",
    "    '소통', '파괴', '신뢰', '대의', '절차', '생명', '석',\n",
    "    '의지', '외', '시청자', '검사', '확대', '탄생','공주','설득','인류','분열',\n",
    "    '모바일','오마이','자연','과','디지털','친','사이트',\n",
    "    '접근', '나이', '개입', '현지', '저희', '각자', '뉴욕타임스', '김현정', '기획',\n",
    "    '중단', '일리', '통', '영어', '너', '낼', '엄마', '공부', '면서','사회'\n",
    "])\n",
    "\n",
    "# 7. 불용어 제거\n",
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t not in domain_stopwords and len(t) > 1]\n",
    "\n",
    "merged_df['nouns_clean'] = merged_df['nouns'].apply(remove_stop)\n",
    "\n",
    "# 8. 결과 확인\n",
    "merged_df[['제목', '본문', 'clean_text', 'nouns', 'nouns_clean']].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450b99a-5d60-425c-82b8-7124f4f0b325",
   "metadata": {},
   "source": [
    "merged_df.to_csv('토픽_분석_결과.csv',index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8995e903-6f3c-4ac2-911b-e5f3b09e9864",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['주토픽'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a96879b-043a-429b-b609-c22a5c3865c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82917bcc-0158-4ac6-8481-fde2cb2300f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c5214-fb6d-4fc0-8d38-2fe46ef47357",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc2a63-1ac1-431c-ac47-0f031c5c30e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 리스트를 문자열로 변환\n",
    "corpus = merged_df['nouns_clean'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# 2. TF-IDF 벡터화 (상위 500개 단어)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 3. 단어별 TF-IDF 점수 정렬\n",
    "tfidf_scores = zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0])\n",
    "sorted_tfidf = sorted(tfidf_scores, key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# 4. 결과 출력 (단어, 점수)\n",
    "for word, score in sorted_tfidf[:300]:  # 상위 50개만 예시로\n",
    "    print(f\"{word}: {score:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e6730-90d7-45be-a26e-dffcd8dfd99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_stopwords = {'따라서', '아마', '갑자기', '별로', '당장', '보통', '무조건', '나은', '직후', '반드시',\n",
    "                        '결코', '적지', '아예', '정작', '이건', '언제', '점점', '하라', '다소', '각각', '말로',\n",
    "                        '왜곡', '일보', '오늘날', '자주', '업체','구조','뮤지컬',\n",
    "                        '영화', '디즈니', '게임', '드라마', '소설', '마블', '애니메이션', '트위터', '캐스팅', '가족',\n",
    "                        '코로나', '페이스북', '아들', '테러', '공연', '뉴욕', '아시아', '독일', '넷플릭스', '결혼',\n",
    "                        '코미디', '유튜브', '연출', '러시아', '만화', '메리', '워싱턴', '뉴시스', '외모', '부모', '협회',\n",
    "                        '뮤지컬', '죽음', '브랜드', '번역', '액션', '백설공주', '동물', '소셜미디어', '업체', '백악관', '의사',\n",
    "                        '소녀', '이정재', '대회', '제다이', '억원', '시즌', '칼럼', '할리', '공동체', '창작', '문장', '고통', '욕망',\n",
    "                        '예능', '남녀', '평점', '경기', '아카데미', '불법', '소비자', '남편', '상처', '화가', '교육감', '질서', '연대',\n",
    "                        '발견', '유명', '경향신문', '올림픽', '그린', '영웅', '시사', '힐러리', '아내', '신청', '공포', '엘리트', '시상식',\n",
    "                        '주연', '조선', '혼란', '저항', '자본주의', '웹툰', '바람', '플로리다', '직업', '취임', '가짜', '아이돌', '경영', '센터',\n",
    "                        '투쟁', '곳곳', '판매', '억압', '윌슨', '극장', '응답', '움직임', '성명', '소식', '기득권', '의문', '정리', '집회', '한계',\n",
    "                        '외면', '적용', '인지', '바다', '피부', '신체', '자녀', '박씨', '관계자', '제품', '개월', '포기', '어머니', '취지', '추가',\n",
    "                        '특파원', '수상', '오후', '플레이', '교체', '동료', '간다', '플랫폼', '스타일', '혁신', '동원', '규모', '미군', '편이', '삭제',\n",
    "                        '편집', '다운', '연결', '파리', '실천', '경계', '대변인', '희생', '지능', '타인', '생산', '반감', '기능', '수용', '정상', '증가',\n",
    "                        '조치', '캘리포니아', '일자리', '약속', '오브', '전개', '하루', '보편', '예측', '특징', '동영상', '위치', '보이지', '구체', '외국인',\n",
    "                        '라이트', '대목', '확보', '홍콩', '훼손', '뿌리', '일각', '요청', '테슬라', '전형', '지식', '상식', '수단', '무기', '거래', '비평',\n",
    "                        '우크라이나', '미투', '에너지', '세계관', '이하', '소속', '실패', '복지', '전반', '촬영', '본질', '살인', '강화', '조건', '과제',\n",
    "                        '대립', '극복', '공동', '팬덤', '권위', '의무', '기자회견', '역대', '감수성', '중앙', '세운', '초반', '통제', '인수', '본격',\n",
    "                        '위험', '도입', '진단', '사안', '폐지', '인구', '부인', '서도', '불평등', '소득', '비용', '대로', '경고', '여부', '비율', '작용',\n",
    "                        '기후', '실현', '답변', '젊은이', '항의', '거론', '대비', '모순', '독립', '구분', '반박', '겨냥', '조지', '우주', '방문', '마스크',\n",
    "                        '진정', '중시', '계급', '개선', '부자', '시점', '구호', '임명', '동맹', '부담', '대책', '통과', '연합', '자료', '블리자드', '성격',\n",
    "                        '멕시코', '계층', '부여', '교과서', '소위', '심리', '형식', '인도', '의심', '격차', '혐의', '장벽', '수익', '지위', '보지', '캡틴',\n",
    "                        '이처럼', '법적', '압박', '단계', '절반', '캠프', '증거', '전면', '게이머', '예전', '지도', '외부', '강제', '수행', '충돌', '대결',\n",
    "                        '의제', '구축', '결론', '재판', '비상', '중도', '임기', '지점', '일베', '구도', '협상', '세금', '박재홍',\n",
    "                        '인어공주', '논란', '공연', '국민', '중국', '원작', '변화', '지지', '반대', '시장', '이상', '대학', '교육', '역사', '지적',\n",
    "                        '혐오', '세대', '영국', '서울', '시즌', '칼럼', '할리', '공동체', '창작', '문장', '고통', '욕망', '예능', '평점', '불법',\n",
    "                        '소비자', '남편', '상처', '화가', '교육감', '질서', '연대', '발견', '유명', '경향신문', '올림픽', '그린', '영웅', '시사', '힐러리',\n",
    "                        '아내', '신청', '공포', '엘리트', '시상식', '주연', '조선', '혼란', '저항', '자본주의', '웹툰', '바람', '플로리다', '직업', '취임',\n",
    "                        '가짜', '아이돌', '경영', '센터', '투쟁', '곳곳', '판매', '억압', '윌슨', '극장', '응답', '움직임', '성명', '소식', '기득권', '의문',\n",
    "                        '정리', '한계', '외면', '적용', '인지', '바다', '피부', '신체', '자녀', '박씨', '관계자', '제품', '개월', '포기', '어머니', '취지',\n",
    "                        '추가', '특파원', '수상', '오후', '플레이', '교체', '동료', '간다', '플랫폼', '스타일', '혁신', '동원', '규모', '미군', '편이', '삭제',\n",
    "                        '편집', '다운', '연결', '파리', '실천', '경계', '대변인', '희생', '지능', '타인', '생산', '반감', '기능', '수용', '정상', '증가',\n",
    "                        '조치', '캘리포니아', '일자리', '약속', '오브', '전개', '하루', '보편', '예측', '특징', '동영상', '위치', '보이지', '구체', '외국인',\n",
    "                        '라이트', '대목', '확보', '홍콩', '훼손', '뿌리', '일각', '요청', '테슬라', '전형', '지식', '상식', '수단', '무기', '거래', '비평',\n",
    "                        '에너지', '세계관', '이하', '소속', '실패', '복지', '전반', '촬영', '본질', '살인', '강화', '조건', '과제', '대립', '극복', '공동',\n",
    "                        '팬덤', '권위', '의무', '기자회견', '역대', '감수성', '중앙', '세운', '초반', '통제', '인수', '본격', '위험', '도입', '진단', '사안',\n",
    "                        '폐지', '인구', '부인', '서도', '소득', '비용', '대로', '경고', '여부', '비율', '작용', '기후', '실현', '답변', '젊은이', '항의',\n",
    "                        '거론', '대비', '모순', '독립', '구분', '반박', '겨냥', '조지', '우주', '방문', '마스크', '진정', '중시', '계급', '개선', '부자',\n",
    "                        '시점', '구호', '임명', '동맹', '부담', '대책', '통과', '연합', '자료', '블리자드', '성격', '멕시코', '계층', '부여', '교과서', '소위',\n",
    "                        '심리', '형식', '인도', '의심', '격차', '장벽', '수익', '지위', '보지', '캡틴', '이처럼', '법적', '압박', '단계', '절반', '캠프',\n",
    "                        '증거', '전면', '게이머', '예전', '지도', '외부', '강제', '수행', '충돌', '대결', '의제', '구축', '결론', '재판', '비상', '중도',\n",
    "                        '임기', '지점', '일베', '구도', '협상', '세금', '박재홍','그다음', '그간', '내내', '부터', '처럼', '뭔가', '어쨌든', '마음껏', '전부',\n",
    "                        '때로는', '최소한', '상대로', '제일', '어제'\n",
    "                       }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eda983-cc19-4301-999f-1c63bd13868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stopwords = additional_stopwords\n",
    "\n",
    "# 불용어 제거\n",
    "merged_df['nouns_clean'] = merged_df['nouns_clean'].apply(\n",
    "    lambda tokens: [t for t in tokens if t not in all_stopwords and len(t) > 1]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e1fdc3-21ab-4240-9efa-f08a3a8b108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 리스트를 문자열로\n",
    "corpus = merged_df['nouns_clean'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# TF-IDF 점수 추출 및 정렬\n",
    "tfidf_scores = zip(vectorizer.get_feature_names_out(), X.sum(axis=0).tolist()[0])\n",
    "sorted_tfidf = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 출력\n",
    "for word, score in sorted_tfidf[:1000]:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e751e82b-4683-4a45-8125-0cadf0c5bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 1) 본문 토큰 컬럼 이름 (실제 컬럼명으로 바꿔주세요)\n",
    "token_col = 'nouns_clean'   \n",
    "\n",
    "# 2) 전체 토큰 리스트\n",
    "all_tokens = []\n",
    "for tokens in merged_df[token_col]:\n",
    "    if isinstance(tokens, list):\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "# 3) 빈도 분석\n",
    "counter = Counter(all_tokens)\n",
    "freq_df = pd.DataFrame(counter.most_common(300), columns=['word', 'count'])\n",
    "print(freq_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df571f9e-918c-4a1e-a08b-328219d54a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display  # 주피터 환경일 경우\n",
    "\n",
    "# 본문 토큰 컬럼명\n",
    "token_col = 'nouns_clean'\n",
    "\n",
    "# 전체 토큰 리스트 수집\n",
    "all_tokens = []\n",
    "for tokens in merged_df[token_col]:\n",
    "    if isinstance(tokens, list):\n",
    "        all_tokens.extend(tokens)\n",
    "\n",
    "# 빈도 분석\n",
    "counter = Counter(all_tokens)\n",
    "freq_df = pd.DataFrame(counter.most_common(300), columns=['word', 'count'])\n",
    "\n",
    "# 전체 출력\n",
    "print(freq_df.to_string(index=False))  # 콘솔용\n",
    "# display(freq_df)                     # Jupyter용\n",
    "\n",
    "# CSV 저장\n",
    "# freq_df.to_csv('word_frequency.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29a561-08bd-47a3-94c9-13250ea7edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) 워드클라우드\n",
    "font_path = 'Library/Fonts/AppleGothic.ttf'\n",
    "\n",
    "wc = WordCloud(\n",
    "    font_path=font_path,\n",
    "    width=800,\n",
    "    height=400,\n",
    "    background_color='white'\n",
    ").generate_from_frequencies(counter)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('키워드 빈도분석')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a53e99-06c7-4f38-9ca7-93fa0b5eb899",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim pyLDAvis numpy pandas matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddee5d6b-5cea-44ed-ac50-797905ad8d96",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "# 로그 무시\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# 전처리된 문서 리스트 예시 (여기에 실제 명사 문서 리스트를 넣으세요)\n",
    "documents = merged_df['nouns_clean'].astype(str).tolist()  # ← 본인의 데이터프레임 열명 사용\n",
    "\n",
    "# TF 벡터화\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Gensim용 사전 및 말뭉치 생성\n",
    "texts = [doc.split() for doc in documents]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 지표 계산 함수들\n",
    "def arun_metric(corpus, dictionary, texts, min_k, max_k):\n",
    "    scores = []\n",
    "    for k in tqdm(range(min_k, max_k+1)):\n",
    "        lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, passes=10)\n",
    "        doc_topic_distr = np.array([[tup[1] for tup in lda[doc]] for doc in corpus])\n",
    "        topic_word_distr = np.array([lda.get_topic_terms(i, topn=len(dictionary)) for i in range(k)])\n",
    "        topic_word_matrix = np.array([[val for _, val in topic] for topic in topic_word_distr])\n",
    "        sv = np.linalg.svd(doc_topic_distr @ topic_word_matrix.T, compute_uv=False)\n",
    "        score = np.sum(np.log(sv))\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def cao_juan_metric(lda_models):\n",
    "    return [np.mean(np.mean(np.dot(model.get_topics(), model.get_topics().T)) - np.eye(model.num_topics)) for model in lda_models]\n",
    "\n",
    "def griffiths_metric(corpus, dictionary, min_k, max_k):\n",
    "    scores = []\n",
    "    for k in tqdm(range(min_k, max_k+1)):\n",
    "        lda = models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, passes=10)\n",
    "        scores.append(lda.log_perplexity(corpus))\n",
    "    return scores\n",
    "\n",
    "def deveaud_metric(lda_models):\n",
    "    from scipy.spatial.distance import jensenshannon\n",
    "    scores = []\n",
    "    for model in lda_models:\n",
    "        sims = []\n",
    "        for i in range(model.num_topics):\n",
    "            for j in range(i+1, model.num_topics):\n",
    "                p = model.get_topic_terms(i, topn=len(dictionary))\n",
    "                q = model.get_topic_terms(j, topn=len(dictionary))\n",
    "                p_vec = np.zeros(len(dictionary))\n",
    "                q_vec = np.zeros(len(dictionary))\n",
    "                for id, val in p:\n",
    "                    p_vec[id] = val\n",
    "                for id, val in q:\n",
    "                    q_vec[id] = val\n",
    "                sims.append(jensenshannon(p_vec, q_vec)**2)\n",
    "        scores.append(np.mean(sims))\n",
    "    return scores\n",
    "\n",
    "# 하이퍼파라미터 범위\n",
    "min_topics = 2\n",
    "max_topics = 30\n",
    "\n",
    "# LDA 모델들 생성\n",
    "lda_models = [\n",
    "    models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=k, passes=10, random_state=42)\n",
    "    for k in tqdm(range(min_topics, max_topics+1))\n",
    "]\n",
    "\n",
    "# 각 지표 계산\n",
    "cao_scores = cao_juan_metric(lda_models)\n",
    "griffiths_scores = griffiths_metric(corpus, dictionary, min_topics, max_topics)\n",
    "deveaud_scores = deveaud_metric(lda_models)\n",
    "# Arun은 계산량이 커서 선택사항\n",
    "# arun_scores = arun_metric(corpus, dictionary, texts, min_topics, max_topics)\n",
    "\n",
    "# 그래프 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = list(range(min_topics, max_topics+1))\n",
    "plt.plot(x, cao_scores, label='CaoJuan2009', marker='o')\n",
    "plt.plot(x, griffiths_scores, label='Griffiths2004', marker='o')\n",
    "plt.plot(x, deveaud_scores, label='Deveaud2014', marker='o')\n",
    "# plt.plot(x, arun_scores, label='Arun2010', marker='o')  # 선택적으로\n",
    "\n",
    "plt.axvline(x=16, color='red', linestyle='--', label='Best topic: 16')\n",
    "plt.legend()\n",
    "plt.title('하이퍼파라미터 튜닝에 의한 모델 성능 지표')\n",
    "plt.xlabel('number of topics')\n",
    "plt.ylabel('score')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bed8ebf-73e9-4073-adc7-9a656a5f5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d2f46-68c2-4fbe-b6ce-7654c9980844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ab18f-46db-4b3e-8c4a-de7ff0be299e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 데이터프레임으로 정리\n",
    "result_df = pd.DataFrame({\n",
    "    'Num Topics': list(topic_range),\n",
    "    'Coherence Score (c_v)': coherence_scores,\n",
    "    'Perplexity Score (log)': perplexity_scores\n",
    "})\n",
    "\n",
    "# Coherence Score 기준 내림차순 정렬\n",
    "result_df_sorted = result_df.sort_values(by='Coherence Score (c_v)', ascending=False)\n",
    "\n",
    "# 전체 출력\n",
    "pd.set_option('display.max_rows', None)\n",
    "print(result_df_sorted.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba9226-6080-4d6c-9c54-7b9c81f61862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f0e0ec-6811-4223-8a8f-96391a31662f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9902866-bb2d-4bb8-9fc9-4b1679b9dce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8241a637-45b7-4350-8f09-9a4718a56942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df834c9b-2753-4dce-abb3-628905d10985",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Coherence\n",
    "plt.plot(topic_range, coherence_scores, marker='o', label='Coherence (c_v)')\n",
    "# Perplexity\n",
    "plt.plot(topic_range, perplexity_scores, marker='x', label='Perplexity')\n",
    "\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Optimal Number of Topics by Coherence and Perplexity\")\n",
    "plt.axvline(x=topic_range[np.argmax(coherence_scores)], color='r', linestyle='--', label=f'Best topic: {topic_range[np.argmax(coherence_scores)]}')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f7d04c-5d18-4a4c-aee2-ca5f028853ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 시각화 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_model,     # 학습된 LDA 모델\n",
    "    corpus=corpus,             # BOW 형식 코퍼스\n",
    "    dictionary=dictionary,     # 사전\n",
    "    sort_topics=False          # 토픽 정렬 유지\n",
    ")\n",
    "\n",
    "# 노트북 환경에서 시각화\n",
    "pyLDAvis.display(vis_data)\n",
    "\n",
    "# 또는 HTML로 저장\n",
    "# pyLDAvis.save_html(vis_data, 'lda_vis.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053d269c-b042-422d-9a47-ecee4e81a069",
   "metadata": {},
   "source": [
    "토픽수 7로 설정하여 다시 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceb6922-dccb-410e-a5b3-ccbe748d9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 텍스트 리스트 및 사전/코퍼스\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_7 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=7,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ba48f-5049-4ca0-8687-5caa6d3cac75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_7,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    sort_topics=False\n",
    ")\n",
    "\n",
    "# Jupyter 환경에서 바로 시각화\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af002f03-e9c8-4b85-98b0-9d36ed9eb7f0",
   "metadata": {},
   "source": [
    "토픽수 8로 설정하여 다시 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc3b98b-50c9-4006-b667-559d479b0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 텍스트 리스트 및 사전/코퍼스\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_8 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=8,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c052d0-a84e-4856-8e37-9b4030da9fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_8,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    sort_topics=False\n",
    ")\n",
    "\n",
    "# Jupyter 환경에서 바로 시각화\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf506ed-5671-45a6-b5d6-d01b5a59851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 텍스트 리스트 및 사전/코퍼스\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_3 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=3,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd571c2-8ee6-46bf-9db1-bdda2220bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_3,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    sort_topics=False\n",
    ")\n",
    "\n",
    "# Jupyter 환경에서 바로 시각화\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378c6ffd-2c08-45b8-a23b-362326e3411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 텍스트 리스트 및 사전/코퍼스\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_4 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=4,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc512e-cc8a-47de-a018-96dbcaf7bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_4,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    sort_topics=False\n",
    ")\n",
    "\n",
    "# Jupyter 환경에서 바로 시각화\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff799ae-abb9-4f1c-b2e9-1eb706921b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 텍스트 리스트 및 사전/코퍼스\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_4 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23edde09-7ee0-45f2-856b-9c9f7180ae69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_4,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    sort_topics=False\n",
    ")\n",
    "\n",
    "# Jupyter 환경에서 바로 시각화\n",
    "pyLDAvis.display(vis_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151fb56c-ed06-491f-8ec9-8d52b38a64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "# 텍스트 리스트 및 사전/코퍼스\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA 모델 학습\n",
    "lda_6 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=6,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f2fae-43c8-4976-8ec0-5d9107ed9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "\n",
    "# pyLDAvis 준비\n",
    "vis_data = gensimvis.prepare(\n",
    "    topic_model=lda_6,\n",
    "    corpus=corpus,\n",
    "    dictionary=dictionary,\n",
    "    sort_topics=False\n",
    ")\n",
    "\n",
    "# Jupyter 환경에서 바로 시각화\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305ca8f8-4143-448e-b311-109fb68ca533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c72587-85ad-454b-bed0-9f2e8d131e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17202ce-1d56-425a-b2f3-929cec6e99aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e46e01-7bc7-48c7-aac8-fc16ca261250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72c8b1-a85a-46f5-ae60-6ca435c9fe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 말뭉치 불러오기\n",
    "#    'nouns_clean' 열이 문자열 형태 리스트라면 ast.literal_eval 적용 필요\n",
    "import ast\n",
    "merged_df['nouns_clean'] = merged_df['nouns_clean'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "\n",
    "# 2) 사전(Dictionary) 및 코퍼스(corpus) 생성\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 3) LDA 모델 재학습 (토픽 수 = 3)\n",
    "lda_3 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=3,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# 4) 토픽별 주요 키워드 추출 (상위 10개)\n",
    "topic_keywords = {\n",
    "    i: [word for word, _ in lda_3.show_topic(i, topn=10)]\n",
    "    for i in range(3)\n",
    "}\n",
    "\n",
    "# 5) 문서별 토픽 배정 (가장 높은 확률 토픽)\n",
    "doc_topics = [\n",
    "    max(lda_3.get_document_topics(bow), key=lambda x: x[1])[0]\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 6) 토픽별 문서 수 집계\n",
    "topic_counts = Counter(doc_topics)\n",
    "\n",
    "# 7) 결과 테이블 생성\n",
    "df_topics = pd.DataFrame([\n",
    "    {\n",
    "        '토픽 번호': i,\n",
    "        '주요 키워드': ', '.join(topic_keywords[i]),\n",
    "        '문서 수': topic_counts.get(i, 0)\n",
    "    }\n",
    "    for i in range(3)\n",
    "])\n",
    "\n",
    "# 8) 문서 수 내림차순 정렬 및 순위 컬럼 추가\n",
    "df_topics = df_topics.sort_values('문서 수', ascending=False).reset_index(drop=True)\n",
    "df_topics.insert(0, '순위', range(1, len(df_topics) + 1))\n",
    "\n",
    "# 9) 출력\n",
    "print(df_topics.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d417daa-395d-4f19-b1fa-243dfef4f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문서별로 가장 높은 확률의 토픽 번호를 '주토픽' 열로 추가\n",
    "merged_df['주토픽'] = doc_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c0d36-ac5e-4324-9ea2-d21974c51550",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = {\n",
    "    i: [word for word, _ in lda_3.show_topic(i, topn=30)]\n",
    "    for i in range(3)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4b5d87-2c52-4fca-a18f-117df838d983",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17dc7df-3f0c-44b9-808e-e436af8dbc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 토픽별 통계 및 대표 제목 추출\n",
    "rows = []\n",
    "for topic_id, info in topic_info.items():\n",
    "    count = len(info['gammas'])\n",
    "    avg_gamma = sum(info['gammas']) / count\n",
    "\n",
    "    # γ값 기준 상위 5개 문서 추출\n",
    "    top5 = sorted(info['docs'], key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "    # 상위 문서의 기사 제목 가져오기\n",
    "    top_titles = [merged_df.loc[i, '제목'] for i, _ in top5]\n",
    "\n",
    "    # 테이블 형태로 저장\n",
    "    rows.append({\n",
    "        '토픽 번호': topic_id + 1,  # 1-based\n",
    "        '문서 수': count,\n",
    "        '평균 γ값': round(avg_gamma, 4),\n",
    "        '대표 기사 제목(상위5)': '\\n'.join(top_titles)\n",
    "    })\n",
    "\n",
    "# 데이터프레임으로 정리\n",
    "df_summary = pd.DataFrame(rows)\n",
    "print(df_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728441c7-1f78-4159-949c-689f7a907e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93ce41b-5c62-4a5b-a8f2-db62aa61fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "# γ값 임계치 설정 (0.8 이상 등)\n",
    "gamma_threshold = 0.8\n",
    "\n",
    "# 토픽별로 γ 높은 문서 인덱스를 저장할 딕셔너리\n",
    "high_gamma_docs = defaultdict(list)\n",
    "\n",
    "# 각 문서별로 γ값을 구해서, 높은 문서를 해당 토픽에 분배\n",
    "for doc_idx, bow in enumerate(corpus):\n",
    "    topic_probs = lda_3.get_document_topics(bow)\n",
    "    for topic_id, gamma in topic_probs:\n",
    "        if gamma >= gamma_threshold:\n",
    "            high_gamma_docs[topic_id].append(doc_idx)\n",
    "\n",
    "# 결과 저장용\n",
    "topic_keywords_summary = []\n",
    "\n",
    "# 각 토픽별로 주요 키워드 추출\n",
    "for topic_id, doc_indices in high_gamma_docs.items():\n",
    "    # 명사 리스트 결합\n",
    "    topic_words = []\n",
    "    for i in doc_indices:\n",
    "        topic_words.extend(merged_df.loc[i, 'nouns_clean'])\n",
    "    \n",
    "    # 상위 키워드 20개 추출\n",
    "    top_keywords = Counter(topic_words).most_common(20)\n",
    "    keyword_list = [kw for kw, _ in top_keywords]\n",
    "\n",
    "    topic_keywords_summary.append({\n",
    "        '토픽 번호': topic_id + 1,  # 1-based\n",
    "        '문서 수': len(doc_indices),\n",
    "        '상위 키워드': ', '.join(keyword_list)\n",
    "    })\n",
    "\n",
    "# 결과 DataFrame\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords_summary)\n",
    "print(df_topic_keywords.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dedea2-0803-49b2-8676-7ffcd10e9c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "gamma_threshold = 0.8  # 필요 없으면 지워도 됨, 어차피 상위 5개 뽑으니까\n",
    "\n",
    "# 토픽별로 (문서번호, γ값) 저장\n",
    "topic_doc_gamma = defaultdict(list)\n",
    "\n",
    "for doc_idx, bow in enumerate(corpus):\n",
    "    topic_probs = lda_3.get_document_topics(bow)\n",
    "    for topic_id, gamma in topic_probs:\n",
    "        topic_doc_gamma[topic_id].append((doc_idx, gamma))\n",
    "\n",
    "# 각 토픽별 상위 5개 문서 뽑기 (γ 기준 내림차순)\n",
    "summary = []\n",
    "\n",
    "for topic_id, doc_gamma_list in topic_doc_gamma.items():\n",
    "    # γ 내림차순 정렬\n",
    "    sorted_docs = sorted(doc_gamma_list, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # 상위 5개만\n",
    "    top_5_docs = sorted_docs[:5]\n",
    "    \n",
    "    for doc_idx, gamma in top_5_docs:\n",
    "        # 주요내용 컬럼 이름은 merged_df에서 주요 텍스트 컬럼명으로 바꿔주세요.\n",
    "        main_content = merged_df.loc[doc_idx, 'text']  # 예시: text 컬럼에 주요내용 있다고 가정\n",
    "        \n",
    "        summary.append({\n",
    "            '토픽': topic_id + 1,\n",
    "            '문서번호': doc_idx,\n",
    "            '주요내용': main_content,\n",
    "            'γ값': round(gamma, 4)\n",
    "        })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "print(df_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb16e76-ae8a-4a47-945e-f985d6f4568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.to_csv('df_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc3c69-5779-4215-b70e-3220d2436db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7638fb-7a38-4d7a-a0e8-6ada8582e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('LDA토픽모델링',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff226c-9ad2-42ab-a6b0-4612c01aec43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c92809-e38d-46fe-8b24-52e58db25eef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9826408-4efd-4574-9d45-e61ce8aa5815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0259a2-5d3f-468a-9cf9-fe2d47c9bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 각 토픽별 상위 5개 단어와 β값 가져오기\n",
    "topic_beta = {\n",
    "    topic_id: lda_3.show_topic(topic_id, topn=5)\n",
    "    for topic_id in range(lda_3.num_topics)\n",
    "}\n",
    "\n",
    "# 2) 데이터프레임으로 변환\n",
    "rows = []\n",
    "for topic_id, terms in topic_beta.items():\n",
    "    for word, beta in terms:\n",
    "        rows.append({\n",
    "            '토픽 번호': topic_id,\n",
    "            '단어': word,\n",
    "            'β 값': round(beta, 6)  # 소수점 6자리로 포맷팅\n",
    "        })\n",
    "\n",
    "df_beta = pd.DataFrame(rows)\n",
    "\n",
    "# 3) 토픽별로 보기 좋게 정렬\n",
    "df_beta = df_beta.sort_values(['토픽 번호', 'β 값'], ascending=[True, False]) \\\n",
    "                 .reset_index(drop=True)\n",
    "\n",
    "# 4) 출력\n",
    "print(df_beta.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f4a134-6000-4f22-a10e-753f97817845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 1) γ값(문서‑토픽 확률) 계산 (minimum_probability=0으로 모든 토픽 확률 반환)\n",
    "doc_topic_dists = [\n",
    "    lda_3.get_document_topics(bow, minimum_probability=0)\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 2) 토픽별로 (문서 인덱스, γ값) 리스트 생성\n",
    "topic_to_docs = defaultdict(list)\n",
    "for doc_idx, dist in enumerate(doc_topic_dists):\n",
    "    for topic_id, gamma in dist:\n",
    "        topic_to_docs[topic_id].append((doc_idx, gamma))\n",
    "\n",
    "# 3) 각 토픽별 γ 기준 상위 5개 문서 선택\n",
    "top5_by_topic = {\n",
    "    topic_id: sorted(docs, key=lambda x: x[1], reverse=True)[:5]\n",
    "    for topic_id, docs in topic_to_docs.items()\n",
    "}\n",
    "\n",
    "# 4) 결과를 담을 DataFrame 생성\n",
    "rows = []\n",
    "for topic_id, docs in top5_by_topic.items():\n",
    "    for doc_idx, gamma in docs:\n",
    "        nouns = merged_df.loc[doc_idx, 'nouns_clean']  # 리스트 형태\n",
    "        rows.append({\n",
    "            '토픽 번호': topic_id,\n",
    "            '문서 인덱스': doc_idx,\n",
    "            'γ 값': round(gamma, 4),\n",
    "            'nouns_clean': nouns\n",
    "        })\n",
    "\n",
    "df_top5_nouns = pd.DataFrame(rows)\n",
    "\n",
    "# 5) 보기 쉽게 정렬\n",
    "df_top5_nouns = df_top5_nouns.sort_values(\n",
    "    ['토픽 번호', 'γ 값'],\n",
    "    ascending=[True, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# 6) 출력\n",
    "print(df_top5_nouns.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e7aa7-05e6-4f24-9955-fcd8cdaaa5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 0) 토픽 수 지정 (예: 16)\n",
    "num_topics = lda_3.num_topics  # 이미 모델에 맞춰져 있다면 이처럼 사용 가능\n",
    "\n",
    "# 1) 모든 문서에 대해 γ 분포 계산 (minimum_probability=0 으로 모든 토픽 확률 포함)\n",
    "doc_topic_dists = [\n",
    "    lda_3.get_document_topics(bow, minimum_probability=0)\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 2) 토픽별 (문서 인덱스, γ값) 리스트 생성\n",
    "topic_to_docs = defaultdict(list)\n",
    "for doc_idx, dist in enumerate(doc_topic_dists):\n",
    "    for topic_id, gamma in dist:\n",
    "        topic_to_docs[topic_id].append((doc_idx, gamma))\n",
    "\n",
    "# 3) 각 토픽별 γ 내림차순 상위 5개 문서 선택\n",
    "top5_by_topic = {\n",
    "    t: sorted(topic_to_docs[t], key=lambda x: x[1], reverse=True)[:5]\n",
    "    for t in range(num_topics)\n",
    "}\n",
    "\n",
    "# 4) 엑셀 파일로 저장하기 위해 각 토픽별 DataFrame 생성\n",
    "with pd.ExcelWriter('topic_top5_nouns.xlsx') as writer:\n",
    "    for topic_id, docs in top5_by_topic.items():\n",
    "        rows = []\n",
    "        for doc_idx, gamma in docs:\n",
    "            nouns = merged_df.loc[doc_idx, 'nouns_clean']\n",
    "            rows.append({\n",
    "                '토픽 번호': topic_id + 1,            # 사람 읽기 편하게 1-based\n",
    "                '문서 인덱스': doc_idx,\n",
    "                'γ 값': round(gamma, 4),\n",
    "                'nouns_clean': ', '.join(nouns)       # 리스트 → 문자열\n",
    "            })\n",
    "        df_topic = pd.DataFrame(rows)\n",
    "        # 시트 이름: \"표7_토픽1\" 등으로 자동 생성\n",
    "        sheet_name = f\"표{7+topic_id}_토픽{topic_id+1}\"\n",
    "        df_topic.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"✅ topic_top5_nouns.xlsx 파일이 생성되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fb5d4-f122-4ed1-8705-cfb8709cdede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# 1) 문서별 토픽 배정 (이미 계산해 두셨다면 이 부분은 생략 가능)\n",
    "#    bow_list = corpus  # 앞서 만드신 corpus\n",
    "#    doc_topics = [max(lda_3.get_document_topics(bow), key=lambda x: x[1])[0] for bow in bow_list]\n",
    "\n",
    "# 2) 토픽별 문서 수 집계\n",
    "topic_counts = Counter(doc_topics)\n",
    "\n",
    "# 3) 전체 문서 수\n",
    "total_docs = len(doc_topics)\n",
    "\n",
    "# 4) 결과 테이블 생성\n",
    "df_overall = pd.DataFrame([\n",
    "    {\n",
    "        '토픽 번호': topic_id + 1,                   # 사람이 보기 편하게 1-based\n",
    "        '문서 수': count,\n",
    "        '비율(%)': round(count / total_docs * 100, 2)\n",
    "    }\n",
    "    for topic_id, count in topic_counts.items()\n",
    "])\n",
    "\n",
    "# 5) 문서 수 내림차순 정렬 및 순위 컬럼 추가\n",
    "df_overall = (\n",
    "    df_overall\n",
    "    .sort_values('문서 수', ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df_overall.insert(0, '순위', df_overall.index + 1)\n",
    "\n",
    "# 6) 출력\n",
    "print(df_overall.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037b94e6-1f2d-412c-976f-30a35ee8b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfc6025-f4b0-4991-aa24-4c1e158d5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1) 한글 폰트 설정 ---\n",
    "# macOS의 경우\n",
    "plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 마이너스 기호 깨짐 방지\n",
    "\n",
    "# --- 2) 스택형 막대차트 그리기 ---\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "# df_year_topic: index=연도, columns=토픽번호, values=평균 γ값\n",
    "df_year_topic.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    ax=ax\n",
    ")\n",
    "\n",
    "ax.set_xlabel('연도', fontsize=12)\n",
    "ax.set_ylabel('평균 γ값 비중', fontsize=12)\n",
    "ax.set_title('연도별 토픽 비중 변화 (2010–2024)', fontsize=14)\n",
    "ax.legend(\n",
    "    title='토픽 번호: β 상위 5개 단어',\n",
    "    bbox_to_anchor=(1.05, 1),\n",
    "    loc='upper left'\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648ee86-9918-476c-bc2d-40fcfa65684d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# 1) 문서별 토픽 γ 분포 계산\n",
    "doc_topic_dists = [\n",
    "    dict(lda_3.get_document_topics(bow, minimum_probability=0))\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 2) merged_df에 γ 분포 컬럼 추가\n",
    "merged_df = merged_df.copy()\n",
    "merged_df['gamma_dist'] = doc_topic_dists\n",
    "\n",
    "# 3) 토픽별 정보 수집\n",
    "topic_info = defaultdict(lambda: {'gammas': [], 'docs': []})\n",
    "\n",
    "for idx, row in merged_df.iterrows():\n",
    "    for topic_id, gamma in row['gamma_dist'].items():\n",
    "        topic_info[topic_id]['gammas'].append(gamma)\n",
    "        topic_info[topic_id]['docs'].append((idx, gamma))\n",
    "\n",
    "# 4) 토픽별 통계 및 대표 제목 추출\n",
    "rows = []\n",
    "for topic_id, info in topic_info.items():\n",
    "    count = len(info['gammas'])\n",
    "    avg_gamma = sum(info['gammas']) / count\n",
    "    # γ값 기준으로 상위 5개 문서 인덱스 뽑기\n",
    "    top5 = sorted(info['docs'], key=lambda x: x[1], reverse=True)[:5]\n",
    "    # 문서 제목 리스트 (컬럼명에 맞게 조정해주세요)\n",
    "    top_titles = [ merged_df.loc[i, '제목'] for i, _ in top5 ]\n",
    "    rows.append({\n",
    "        '토픽 번호': topic_id + 1,               # 1‑based 번호\n",
    "        '문서 수': count,\n",
    "        '평균 γ값': round(avg_gamma, 4),\n",
    "        '대표 기사 제목(상위5)': '\\n'.join(top_titles)\n",
    "    })\n",
    "\n",
    "# 5) DataFrame 생성 및 정렬\n",
    "df_summary = pd.DataFrame(rows)\n",
    "df_summary = df_summary.sort_values('문서 수', ascending=False).reset_index(drop=True)\n",
    "df_summary.insert(0, '순위', df_summary.index + 1)\n",
    "\n",
    "# 6) 출력\n",
    "print(df_summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc31545-6cfa-49a0-815e-8289f2a48465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# 문서별 토픽 분포 (γ값)\n",
    "doc_topic_dists = [\n",
    "    dict(lda_3.get_document_topics(bow, minimum_probability=0.0))\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 토픽 수\n",
    "num_topics = lda_3.num_topics\n",
    "\n",
    "# 각 토픽별 γ값 누적\n",
    "topic_gamma = defaultdict(list)\n",
    "\n",
    "# 각 문서에서 모든 토픽의 γ값 누적\n",
    "for dist in doc_topic_dists:\n",
    "    for topic_id in range(num_topics):\n",
    "        gamma = dist.get(topic_id, 0.0)\n",
    "        topic_gamma[topic_id].append(gamma)\n",
    "\n",
    "# 평균 γ값 계산\n",
    "topic_avg_gamma = {\n",
    "    topic_id: round(sum(gammas) / len(gammas), 4)\n",
    "    for topic_id, gammas in topic_gamma.items()\n",
    "}\n",
    "\n",
    "# 정리된 결과를 DataFrame으로 보기 좋게 출력\n",
    "df_avg_gamma = pd.DataFrame([\n",
    "    {\"토픽 번호\": topic_id + 1, \"평균 토픽 소속 확률 (γ)\": gamma}\n",
    "    for topic_id, gamma in sorted(topic_avg_gamma.items())\n",
    "])\n",
    "\n",
    "print(df_avg_gamma.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90105969-ed64-4aba-acd6-2b9a3441b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# 문서별 연도 정보가 들어 있는 리스트 (예: [2020, 2020, 2021, ...])\n",
    "# 문서 순서와 corpus의 순서가 같아야 함\n",
    "years = df_overall[\"연도\"].tolist()  # 또는 따로 리스트로 보관된 경우 그대로 사용\n",
    "\n",
    "# 문서별 토픽 분포 (γ값 리스트)\n",
    "doc_topic_dists = [\n",
    "    dict(lda_3.get_document_topics(bow, minimum_probability=0.0))\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 문서별 주토픽 추출\n",
    "dominant_topics = [\n",
    "    max(dist.items(), key=lambda x: x[1])[0]  # γ값이 가장 큰 토픽\n",
    "    for dist in doc_topic_dists\n",
    "]\n",
    "\n",
    "# 연도별 주토픽 카운트\n",
    "year_topic_counts = defaultdict(lambda: defaultdict(int))\n",
    "for year, topic in zip(years, dominant_topics):\n",
    "    year_topic_counts[year][topic] += 1\n",
    "\n",
    "# DataFrame으로 변환\n",
    "df_topic_by_year = pd.DataFrame(year_topic_counts).fillna(0).astype(int).T\n",
    "df_topic_by_year.columns = [f\"Topic {i+1}\" for i in df_topic_by_year.columns]\n",
    "\n",
    "# 시각화\n",
    "df_topic_by_year.plot(kind='line', figsize=(12, 6), marker='o')\n",
    "plt.title(\"연도별 주토픽 분포\")\n",
    "plt.xlabel(\"연도\")\n",
    "plt.ylabel(\"문서 수\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title=\"토픽\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0209f33-c23f-4467-99a4-067435c8e2b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec42b4d-35a9-4d30-aa54-e918db924613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 문서별 γ 값 기반으로 주 토픽 추출\n",
    "doc_topics = []\n",
    "for i, doc_bow in enumerate(corpus):\n",
    "    topic_dist = lda_model.get_document_topics(doc_bow)\n",
    "    sorted_topic = sorted(topic_dist, key=lambda x: x[1], reverse=True)\n",
    "    main_topic = sorted_topic[0][0]  # 가장 확률 높은 토픽\n",
    "    main_prob = sorted_topic[0][1]   # 해당 확률\n",
    "    doc_topics.append((main_topic, main_prob))\n",
    "\n",
    "# 2. merged_df에 주 토픽 및 확률 열 추가\n",
    "merged_df['주토픽'] = [topic for topic, prob in doc_topics]\n",
    "merged_df['토픽확률'] = [prob for topic, prob in doc_topics]\n",
    "\n",
    "# (선택) 주토픽별 기사 수 확인\n",
    "topic_counts = merged_df['주토픽'].value_counts().sort_index()\n",
    "print(\"토픽별 기사 수:\\n\", topic_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50078e3e-baa5-416c-b6b0-af4d5e71fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 토픽별 기사 수 계산\n",
    "topic_counts = merged_df['주토픽'].value_counts().sort_index()\n",
    "\n",
    "# 2. 200개 이상인 토픽만 필터링\n",
    "topics_over_200 = topic_counts[topic_counts >= 200]\n",
    "\n",
    "# 3. 출력\n",
    "print(\"✅ 200개 이상 기사 보유 토픽:\")\n",
    "print(topics_over_200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c697d04-38ef-4c97-a2e9-981eeee41ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 기사 수 기준 필터링\n",
    "topic_counts = merged_df['주토픽'].value_counts().sort_index()\n",
    "topics_over_200 = topic_counts[topic_counts >= 200]\n",
    "\n",
    "# 2. 토픽 번호를 DataFrame으로 변환\n",
    "df_over_200 = topics_over_200.reset_index()\n",
    "df_over_200.columns = ['토픽 번호', '기사 수']\n",
    "\n",
    "# 3. 토픽 키워드와 병합\n",
    "df_topics_summary = pd.merge(df_over_200, df_topics[['토픽 번호', '주요 키워드']], on='토픽 번호', how='left')\n",
    "\n",
    "# 4. 출력\n",
    "print(df_topics_summary.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc1dbb5-ed14-45c9-8df8-0f4063c89456",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_topics.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817aade3-2a1f-440d-aa2a-4250285ac536",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3006958f-961a-4524-878c-0687766d30a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce274413-1f60-441c-95bf-6b057e6fdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. 주토픽 기준으로 기사 수 세기\n",
    "topic_counts = merged_df['주토픽'].value_counts().reset_index()\n",
    "topic_counts.columns = ['토픽 번호', '문서 수']\n",
    "topic_counts = topic_counts.sort_values('토픽 번호').reset_index(drop=True)\n",
    "\n",
    "# 2. 각 토픽의 상위 키워드 추출 (Top 10)\n",
    "topics_keywords = []\n",
    "for i in range(lda_model.num_topics):\n",
    "    terms = lda_model.show_topic(i, topn=10)\n",
    "    keywords = \", \".join([term for term, weight in terms])\n",
    "    topics_keywords.append({'토픽 번호': i, '주요 키워드': keywords})\n",
    "\n",
    "df_keywords = pd.DataFrame(topics_keywords)\n",
    "\n",
    "# 3. 키워드 + 문서 수를 토픽 번호 기준으로 merge\n",
    "df_topics = pd.merge(topic_counts, df_keywords, on='토픽 번호', how='left')\n",
    "\n",
    "# 4. 정렬 및 순위 부여\n",
    "df_topics = df_topics.sort_values('문서 수', ascending=False).reset_index(drop=True)\n",
    "df_topics.insert(0, '순위', range(1, len(df_topics) + 1))\n",
    "\n",
    "# 5. 출력\n",
    "print(df_topics[['순위', '토픽 번호', '주요 키워드', '문서 수']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e3325-322e-4209-a4ea-4c6f77f899e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066197b1-ddaa-4ce4-8a13-972f0d69e470",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66bbc2-fd57-4128-9ee7-95e2988150d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9794df-81d0-41b2-b479-74a2c693e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df[['제목','본문','nouns_clean','gamma_dist','주토픽','토픽확률']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659e39a-d86e-4c42-9659-ed0be977291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adade27-705e-4b83-ac07-e84f057962d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2.to_csv('merged2_df.csv',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c88ad-663e-48fb-9a66-989d80590c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('토픽_분석_결과.csv',index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627816a0-2cd8-43ee-8fae-d3c050e18297",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dcd954-5304-485c-8267-72a2fb2aef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "merged_df = pd.read_csv('토픽_분석_결과.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4485b01-0f10-4fe6-8061-06c1e036c3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# 1) 말뭉치 불러오기\n",
    "merged_df['nouns_clean'] = merged_df['nouns_clean'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "\n",
    "# 2) 사전(Dictionary) 및 코퍼스 생성\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 3) LDA 모델 학습 (토픽 수 = 5)\n",
    "lda_5 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=5,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# 4) 토픽별 주요 키워드 추출 (상위 10개)\n",
    "topic_keywords = {\n",
    "    i: [word for word, _ in lda_5.show_topic(i, topn=10)]\n",
    "    for i in range(5)  # ✅ 수정: range(5)\n",
    "}\n",
    "\n",
    "# 5) 문서별 토픽 배정\n",
    "doc_topics = [\n",
    "    max(lda_5.get_document_topics(bow), key=lambda x: x[1])[0]\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 6) 토픽별 문서 수 집계\n",
    "topic_counts = Counter(doc_topics)\n",
    "\n",
    "# 7) 결과 테이블 생성\n",
    "df_topics = pd.DataFrame([\n",
    "    {\n",
    "        '토픽 번호': i,\n",
    "        '주요 키워드': ', '.join(topic_keywords[i]),\n",
    "        '문서 수': topic_counts.get(i, 0)\n",
    "    }\n",
    "    for i in range(5)\n",
    "])\n",
    "\n",
    "# 8) 문서 수 기준 정렬 및 순위 부여\n",
    "df_topics = df_topics.sort_values('문서 수', ascending=False).reset_index(drop=True)\n",
    "df_topics.insert(0, '순위', range(1, len(df_topics) + 1))\n",
    "\n",
    "# 9) 출력\n",
    "print(df_topics.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1323733b-d40a-4aec-b149-653058a17c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "import ast\n",
    "\n",
    "# 1) 말뭉치 준비\n",
    "merged_df['nouns_clean'] = merged_df['nouns_clean'].apply(\n",
    "    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    ")\n",
    "texts = merged_df['nouns_clean'].tolist()\n",
    "\n",
    "# 2) 사전 및 코퍼스 생성\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 3) LDA 모델 학습 (토픽 수 = 3)\n",
    "lda_3 = models.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=3,\n",
    "    random_state=42,\n",
    "    passes=10,\n",
    "    iterations=100,\n",
    "    alpha='auto',\n",
    "    eta='auto'\n",
    ")\n",
    "\n",
    "# 4) 문서별 주토픽 추출\n",
    "dominant_topics = [\n",
    "    max(lda_3.get_document_topics(bow), key=lambda x: x[1])[0]\n",
    "    for bow in corpus\n",
    "]\n",
    "\n",
    "# 5) 주토픽을 데이터프레임에 병합\n",
    "merged_df['주토픽'] = dominant_topics\n",
    "\n",
    "# 6) 연도별 주토픽 분포 집계 (비율 기준)\n",
    "topic_by_year = (\n",
    "    merged_df.groupby(['연도', '주토픽'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .pivot(index='연도', columns='주토픽', values='count')\n",
    "    .fillna(0)\n",
    ")\n",
    "\n",
    "# 비율로 변환\n",
    "topic_by_year_ratio = topic_by_year.div(topic_by_year.sum(axis=1), axis=0)\n",
    "\n",
    "# 7) 시각화 (누적 막대그래프)\n",
    "topic_by_year_ratio.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    colormap='tab10'\n",
    ")\n",
    "\n",
    "plt.title('연도별 주토픽 분포 비율 (LDA: 3 토픽)')\n",
    "plt.ylabel('비율')\n",
    "plt.xlabel('연도')\n",
    "plt.legend(title='토픽 번호', loc='upper right')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70000bd0-abc8-4e40-9aad-c61d5091661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 1. AppleGothic 폰트 설정\n",
    "font_path = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path).get_name()\n",
    "plt.rc('font', family=fontprop)\n",
    "\n",
    "\n",
    "# 마이너스 깨짐 방지\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "topic_by_year_ratio.plot(\n",
    "    kind='bar',\n",
    "    stacked=True,\n",
    "    figsize=(10, 6),\n",
    "    colormap='tab10'\n",
    ")\n",
    "\n",
    "plt.title('연도별 주토픽 분포 비율 (LDA: 3 토픽)')\n",
    "plt.ylabel('비율')\n",
    "plt.xlabel('연도')\n",
    "plt.legend(title='토픽 번호', loc='upper right')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992f556-7097-4650-8035-33371e301143",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_by_year = (\n",
    "    merged_df.groupby(['연도', '주토픽'])\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    "    .pivot(index='연도', columns='주토픽', values='count')\n",
    "    .fillna(0)\n",
    ")\n",
    "topic_by_year_ratio = topic_by_year.div(topic_by_year.sum(axis=1), axis=0)\n",
    "\n",
    "# 6. 수치만 출력\n",
    "print(topic_by_year_ratio.round(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe8128-800b-4585-b255-8c00546e48c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
