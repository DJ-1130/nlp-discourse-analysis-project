{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d3ac63-6061-4549-9c4c-93dd6a6d8377",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8fdf04d-22f2-48d1-b862-e57e4750fb0b",
   "metadata": {},
   "source": [
    "본 코드는 토픽모델링\u0000 및\u0000 감성분석\u0000 기법을\u0000 활용한\u0000 언론\u0000 기사의\u0000 핵심\u0000 이슈\u0000 및\u0000 논조\u0000 분석 프로젝트를 위해 필요한 데이터를 크롤링한 코드이다.해당 프로젝트에 필요한 데이터는 뉴스기사 텍스트 데이터로, selenuim을 이용하여 다음 뉴스기사에서 2010년부터 2024년까지의 뉴스 데이터 중 \"정치적 올바름\" 키워드를 포함한 뉴스기사만을 추출하여 '순번', '제목', '본문', '신문사', '연도'을 파싱하고 데이터프레임 형태로 변환하여 csv 파일로 저장하였다. 2010년부터 2016년의 기사데이터가 다른 연도의 1년치와 그 수가 비슷하여 하나의 데이터프레임으로 병합하였다. 포털을 일반적으로 많이 사용하는 네이버 뉴스가 아닌 다음뉴스 url로 진행한 이유는 네이버는 언론사 코드와 기사 번호를 포함한 복잡한 경로와 파라미터를 사용하는 반면, 다음은 날짜 기반의 간결한 구조를 채택하고 있다. 본문까지 추출해야하는 이번 크롤링의 경우 신문사별로 링크가 구분되지 않고 통일된 다음 뉴스를 사용하는 것이 필요한 html 태그를 파싱하는 데도 간편했기 때문이다. 하지만 네이버와 달리 다음 뉴스는 페이지네이션 방식이 무한스크롤이 아니어서 1페이지씩 넘겨야한다는 특"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092922e-9f5b-433a-82a7-a4ba6abcd608",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf18f07-785e-4c3b-8c8a-48ca3b32cb54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10201,
     "status": "ok",
     "timestamp": 1746359554197,
     "user": {
      "displayName": "김형주",
      "userId": "03170089748317362301"
     },
     "user_tz": -540
    },
    "id": "8bf18f07-785e-4c3b-8c8a-48ca3b32cb54",
    "outputId": "62c9980a-147b-4ed7-d0de-3bd62dd486d0"
   },
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f464dc-43f6-4bfa-acb1-1d5ab24b2466",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5230,
     "status": "ok",
     "timestamp": 1746359559428,
     "user": {
      "displayName": "김형주",
      "userId": "03170089748317362301"
     },
     "user_tz": -540
    },
    "id": "13f464dc-43f6-4bfa-acb1-1d5ab24b2466",
    "outputId": "f647ec90-d17b-47a7-d021-290dfd83b436"
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda19a8-cd0d-47a7-967e-6343485d3bbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4048,
     "status": "ok",
     "timestamp": 1746359563481,
     "user": {
      "displayName": "김형주",
      "userId": "03170089748317362301"
     },
     "user_tz": -540
    },
    "id": "3fda19a8-cd0d-47a7-967e-6343485d3bbc",
    "outputId": "a75ee4d6-19ff-43a1-f685-c8beaf45920c"
   },
   "outputs": [],
   "source": [
    "pip install chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9fa85-c035-49b4-8e67-b784bb9f5a70",
   "metadata": {
    "id": "31f9fa85-c035-49b4-8e67-b784bb9f5a70"
   },
   "source": [
    "크롤링 2010.01.01 ~ 2016.12.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe9d50-7ffb-4334-8209-97b002d7954a",
   "metadata": {
    "id": "0bbe9d50-7ffb-4334-8209-97b002d7954a",
    "outputId": "260c1ac1-d683-4b49-c738-15e5d5f37969"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for pg in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={pg}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(1.0)  # 대기 시간 증가\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "\n",
    "        print(f\"▶ p={pg}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "\n",
    "        # 중복 기사만 나와도 계속 진행\n",
    "        # 중단 조건 제거\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20100101000000\",\n",
    "    end_date=\"20161231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    # 1) ID 수집\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    # 2) 본문 파싱\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20100101000000\",\n",
    "        end_date=\"20161231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2010_2016_pc_full.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2010_2016_pc_full.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebdb45-80bc-4d0a-aca4-5c5ad23b2413",
   "metadata": {
    "id": "3eebdb45-80bc-4d0a-aca4-5c5ad23b2413"
   },
   "source": [
    "크롤링 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437eefb-f4b1-4fbd-bb3d-7c412a051491",
   "metadata": {
    "id": "8437eefb-f4b1-4fbd-bb3d-7c412a051491",
    "outputId": "3fa35ae8-f187-4c6a-a003-f2afe9bfd6f4"
   },
   "outputs": [],
   "source": [
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for pg in range(1, max_pages+1):\n",
    "        url = f\"{base}&p={pg}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={pg}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and pg > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20170101000000\",\n",
    "    end_date=\"20171231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번','제목','본문','신문사','연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20170101000000\",\n",
    "        end_date=\"20171231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2017_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2017_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78442a-35b6-4264-926f-47602048c21e",
   "metadata": {
    "id": "6b78442a-35b6-4264-926f-47602048c21e"
   },
   "source": [
    "크롤링 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b422070-9148-4480-ab6b-3c94339318ee",
   "metadata": {
    "id": "5b422070-9148-4480-ab6b-3c94339318ee",
    "outputId": "7ccc5eb0-cab6-4f91-98ed-ea73d310b9be"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for pg in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={pg}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={pg}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and pg > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20180101000000\",\n",
    "    end_date=\"20181231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20180101000000\",\n",
    "        end_date=\"20181231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2018_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2018_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f5500-a256-440a-bf2b-2fdc40486c03",
   "metadata": {
    "id": "6b5f5500-a256-440a-bf2b-2fdc40486c03"
   },
   "source": [
    "크롤링 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028c87e-7d45-4cc9-9124-0d89cbcde642",
   "metadata": {
    "id": "f028c87e-7d45-4cc9-9124-0d89cbcde642",
    "outputId": "951d78f0-ed48-48b6-d25b-ddafdc0966f4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={p}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20190101000000\",\n",
    "    end_date=\"20191231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20190101000000\",\n",
    "        end_date=\"20191231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2019_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2019_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d83608-44ac-4bb9-a256-9f32b98ceb8e",
   "metadata": {
    "id": "93d83608-44ac-4bb9-a256-9f32b98ceb8e"
   },
   "source": [
    "크롤링 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21756a95-8398-4448-9976-03d710a0aded",
   "metadata": {
    "id": "21756a95-8398-4448-9976-03d710a0aded",
    "outputId": "c91efd8c-e9f8-4de4-e437-2c7bc0d0920b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={p}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20200101000000\",\n",
    "    end_date=\"20201231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20200101000000\",\n",
    "        end_date=\"20201231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2020_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2020_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea1192-1c11-4a78-a32d-7ad539e688a8",
   "metadata": {
    "id": "0eea1192-1c11-4a78-a32d-7ad539e688a8"
   },
   "source": [
    "크롤링 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373001dc-0707-40b5-9831-3c84d1b18222",
   "metadata": {
    "id": "373001dc-0707-40b5-9831-3c84d1b18222"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"  # ✅ URL 수정 부분\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={p}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=(5,10))\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20210101000000\",   # ✅ 2021년으로 변경\n",
    "    end_date=\"20211231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20210101000000\",   # ✅ 2021년 시작일\n",
    "        end_date=\"20211231235959\",     # ✅ 2021년 종료일\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2021_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")  # ✅ 저장 파일명도 2021로\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2021_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c18ac-cc45-4f0f-bad0-65489bc569ea",
   "metadata": {
    "id": "f15c18ac-cc45-4f0f-bad0-65489bc569ea"
   },
   "source": [
    "크롤링 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bedc91-9a00-4404-911b-350eb8f23c51",
   "metadata": {
    "id": "b1bedc91-9a00-4404-911b-350eb8f23c51"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"  # ✅ 클러스터 옵션 추가\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={p}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20220101000000\",   # ✅ 2022년 시작일\n",
    "    end_date=\"20221231235959\",     # ✅ 2022년 종료일\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20220101000000\",   # ✅ 2022년 시작일\n",
    "        end_date=\"20221231235959\",     # ✅ 2022년 종료일\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2022_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")  # ✅ 저장 파일명도 2022로\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2022_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67b3d0-33c5-4843-b00b-a2c07f45dc41",
   "metadata": {
    "id": "ad67b3d0-33c5-4843-b00b-a2c07f45dc41"
   },
   "source": [
    "크롤링 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993278cf-50ab-4c05-b4b1-ea79ba2e9453",
   "metadata": {
    "id": "993278cf-50ab-4c05-b4b1-ea79ba2e9453"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"  # ✅ 클러스터 옵션 포함\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"▶ p={p}: 신규 {new}건, 누적 {len(ids)}건\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"정치적 올바름\",\n",
    "    start_date=\"20230101000000\",   # ✅ 2023년 시작일\n",
    "    end_date=\"20231231235959\",     # ✅ 2023년 종료일\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\n🔖 파싱 대상 ID: {len(ids)}건\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                '순번': idx,\n",
    "                '제목': title,\n",
    "                '본문': content,\n",
    "                '신문사': press,\n",
    "                '연도': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ [{aid}] 파싱 실패:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['순번', '제목', '본문', '신문사', '연도'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"정치적 올바름\",\n",
    "        start_date=\"20230101000000\",   # ✅ 2023년 시작일\n",
    "        end_date=\"20231231235959\",     # ✅ 2023년 종료일\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2023_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")  # ✅ 저장 파일명도 2023으로\n",
    "    print(f\"✅ 총 {len(df)}건 크롤링 완료 — daum_2023_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd6c21-e41c-42f5-bec4-ac0ff75e7cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
