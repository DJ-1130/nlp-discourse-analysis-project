{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d3ac63-6061-4549-9c4c-93dd6a6d8377",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8fdf04d-22f2-48d1-b862-e57e4750fb0b",
   "metadata": {},
   "source": [
    "ë³¸ ì½”ë“œëŠ” í† í”½ëª¨ë¸ë§\u0000 ë°\u0000 ê°ì„±ë¶„ì„\u0000 ê¸°ë²•ì„\u0000 í™œìš©í•œ\u0000 ì–¸ë¡ \u0000 ê¸°ì‚¬ì˜\u0000 í•µì‹¬\u0000 ì´ìŠˆ\u0000 ë°\u0000 ë…¼ì¡°\u0000 ë¶„ì„ í”„ë¡œì íŠ¸ë¥¼ ìœ„í•´ í•„ìš”í•œ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•œ ì½”ë“œì´ë‹¤.í•´ë‹¹ í”„ë¡œì íŠ¸ì— í•„ìš”í•œ ë°ì´í„°ëŠ” ë‰´ìŠ¤ê¸°ì‚¬ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ, selenuimì„ ì´ìš©í•˜ì—¬ ë‹¤ìŒ ë‰´ìŠ¤ê¸°ì‚¬ì—ì„œ 2010ë…„ë¶€í„° 2024ë…„ê¹Œì§€ì˜ ë‰´ìŠ¤ ë°ì´í„° ì¤‘ \"ì •ì¹˜ì  ì˜¬ë°”ë¦„\" í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ë‰´ìŠ¤ê¸°ì‚¬ë§Œì„ ì¶”ì¶œí•˜ì—¬ 'ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'ì„ íŒŒì‹±í•˜ê³  ë°ì´í„°í”„ë ˆì„ í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ csv íŒŒì¼ë¡œ ì €ì¥í•˜ì˜€ë‹¤. 2010ë…„ë¶€í„° 2016ë…„ì˜ ê¸°ì‚¬ë°ì´í„°ê°€ ë‹¤ë¥¸ ì—°ë„ì˜ 1ë…„ì¹˜ì™€ ê·¸ ìˆ˜ê°€ ë¹„ìŠ·í•˜ì—¬ í•˜ë‚˜ì˜ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³‘í•©í•˜ì˜€ë‹¤. í¬í„¸ì„ ì¼ë°˜ì ìœ¼ë¡œ ë§ì´ ì‚¬ìš©í•˜ëŠ” ë„¤ì´ë²„ ë‰´ìŠ¤ê°€ ì•„ë‹Œ ë‹¤ìŒë‰´ìŠ¤ urlë¡œ ì§„í–‰í•œ ì´ìœ ëŠ” ë„¤ì´ë²„ëŠ” ì–¸ë¡ ì‚¬ ì½”ë“œì™€ ê¸°ì‚¬ ë²ˆí˜¸ë¥¼ í¬í•¨í•œ ë³µì¡í•œ ê²½ë¡œì™€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°˜ë©´, ë‹¤ìŒì€ ë‚ ì§œ ê¸°ë°˜ì˜ ê°„ê²°í•œ êµ¬ì¡°ë¥¼ ì±„íƒí•˜ê³  ìˆë‹¤. ë³¸ë¬¸ê¹Œì§€ ì¶”ì¶œí•´ì•¼í•˜ëŠ” ì´ë²ˆ í¬ë¡¤ë§ì˜ ê²½ìš° ì‹ ë¬¸ì‚¬ë³„ë¡œ ë§í¬ê°€ êµ¬ë¶„ë˜ì§€ ì•Šê³  í†µì¼ëœ ë‹¤ìŒ ë‰´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ í•„ìš”í•œ html íƒœê·¸ë¥¼ íŒŒì‹±í•˜ëŠ” ë°ë„ ê°„í¸í–ˆê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ë„¤ì´ë²„ì™€ ë‹¬ë¦¬ ë‹¤ìŒ ë‰´ìŠ¤ëŠ” í˜ì´ì§€ë„¤ì´ì…˜ ë°©ì‹ì´ ë¬´í•œìŠ¤í¬ë¡¤ì´ ì•„ë‹ˆì–´ì„œ 1í˜ì´ì§€ì”© ë„˜ê²¨ì•¼í•œë‹¤ëŠ” íŠ¹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092922e-9f5b-433a-82a7-a4ba6abcd608",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf18f07-785e-4c3b-8c8a-48ca3b32cb54",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10201,
     "status": "ok",
     "timestamp": 1746359554197,
     "user": {
      "displayName": "ê¹€í˜•ì£¼",
      "userId": "03170089748317362301"
     },
     "user_tz": -540
    },
    "id": "8bf18f07-785e-4c3b-8c8a-48ca3b32cb54",
    "outputId": "62c9980a-147b-4ed7-d0de-3bd62dd486d0"
   },
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f464dc-43f6-4bfa-acb1-1d5ab24b2466",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5230,
     "status": "ok",
     "timestamp": 1746359559428,
     "user": {
      "displayName": "ê¹€í˜•ì£¼",
      "userId": "03170089748317362301"
     },
     "user_tz": -540
    },
    "id": "13f464dc-43f6-4bfa-acb1-1d5ab24b2466",
    "outputId": "f647ec90-d17b-47a7-d021-290dfd83b436"
   },
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda19a8-cd0d-47a7-967e-6343485d3bbc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4048,
     "status": "ok",
     "timestamp": 1746359563481,
     "user": {
      "displayName": "ê¹€í˜•ì£¼",
      "userId": "03170089748317362301"
     },
     "user_tz": -540
    },
    "id": "3fda19a8-cd0d-47a7-967e-6343485d3bbc",
    "outputId": "a75ee4d6-19ff-43a1-f685-c8beaf45920c"
   },
   "outputs": [],
   "source": [
    "pip install chromedriver-autoinstaller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f9fa85-c035-49b4-8e67-b784bb9f5a70",
   "metadata": {
    "id": "31f9fa85-c035-49b4-8e67-b784bb9f5a70"
   },
   "source": [
    "í¬ë¡¤ë§ 2010.01.01 ~ 2016.12.31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbe9d50-7ffb-4334-8209-97b002d7954a",
   "metadata": {
    "id": "0bbe9d50-7ffb-4334-8209-97b002d7954a",
    "outputId": "260c1ac1-d683-4b49-c738-15e5d5f37969"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for pg in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={pg}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(1.0)  # ëŒ€ê¸° ì‹œê°„ ì¦ê°€\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "\n",
    "        print(f\"â–¶ p={pg}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "\n",
    "        # ì¤‘ë³µ ê¸°ì‚¬ë§Œ ë‚˜ì™€ë„ ê³„ì† ì§„í–‰\n",
    "        # ì¤‘ë‹¨ ì¡°ê±´ ì œê±°\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20100101000000\",\n",
    "    end_date=\"20161231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    # 1) ID ìˆ˜ì§‘\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    # 2) ë³¸ë¬¸ íŒŒì‹±\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20100101000000\",\n",
    "        end_date=\"20161231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2010_2016_pc_full.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2010_2016_pc_full.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eebdb45-80bc-4d0a-aca4-5c5ad23b2413",
   "metadata": {
    "id": "3eebdb45-80bc-4d0a-aca4-5c5ad23b2413"
   },
   "source": [
    "í¬ë¡¤ë§ 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437eefb-f4b1-4fbd-bb3d-7c412a051491",
   "metadata": {
    "id": "8437eefb-f4b1-4fbd-bb3d-7c412a051491",
    "outputId": "3fa35ae8-f187-4c6a-a003-f2afe9bfd6f4"
   },
   "outputs": [],
   "source": [
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for pg in range(1, max_pages+1):\n",
    "        url = f\"{base}&p={pg}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={pg}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and pg > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20170101000000\",\n",
    "    end_date=\"20171231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ','ì œëª©','ë³¸ë¬¸','ì‹ ë¬¸ì‚¬','ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20170101000000\",\n",
    "        end_date=\"20171231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2017_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2017_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b78442a-35b6-4264-926f-47602048c21e",
   "metadata": {
    "id": "6b78442a-35b6-4264-926f-47602048c21e"
   },
   "source": [
    "í¬ë¡¤ë§ 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b422070-9148-4480-ab6b-3c94339318ee",
   "metadata": {
    "id": "5b422070-9148-4480-ab6b-3c94339318ee",
    "outputId": "7ccc5eb0-cab6-4f91-98ed-ea73d310b9be"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for pg in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={pg}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={pg}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and pg > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20180101000000\",\n",
    "    end_date=\"20181231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20180101000000\",\n",
    "        end_date=\"20181231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2018_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2018_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f5500-a256-440a-bf2b-2fdc40486c03",
   "metadata": {
    "id": "6b5f5500-a256-440a-bf2b-2fdc40486c03"
   },
   "source": [
    "í¬ë¡¤ë§ 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028c87e-7d45-4cc9-9124-0d89cbcde642",
   "metadata": {
    "id": "f028c87e-7d45-4cc9-9124-0d89cbcde642",
    "outputId": "951d78f0-ed48-48b6-d25b-ddafdc0966f4"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={p}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20190101000000\",\n",
    "    end_date=\"20191231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20190101000000\",\n",
    "        end_date=\"20191231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2019_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2019_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d83608-44ac-4bb9-a256-9f32b98ceb8e",
   "metadata": {
    "id": "93d83608-44ac-4bb9-a256-9f32b98ceb8e"
   },
   "source": [
    "í¬ë¡¤ë§ 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21756a95-8398-4448-9976-03d710a0aded",
   "metadata": {
    "id": "21756a95-8398-4448-9976-03d710a0aded",
    "outputId": "c91efd8c-e9f8-4de4-e437-2c7bc0d0920b"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={p}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20200101000000\",\n",
    "    end_date=\"20201231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20200101000000\",\n",
    "        end_date=\"20201231235959\",\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2020_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2020_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea1192-1c11-4a78-a32d-7ad539e688a8",
   "metadata": {
    "id": "0eea1192-1c11-4a78-a32d-7ad539e688a8"
   },
   "source": [
    "í¬ë¡¤ë§ 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373001dc-0707-40b5-9831-3c84d1b18222",
   "metadata": {
    "id": "373001dc-0707-40b5-9831-3c84d1b18222"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"  # âœ… URL ìˆ˜ì • ë¶€ë¶„\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={p}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=(5,10))\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20210101000000\",   # âœ… 2021ë…„ìœ¼ë¡œ ë³€ê²½\n",
    "    end_date=\"20211231235959\",\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20210101000000\",   # âœ… 2021ë…„ ì‹œì‘ì¼\n",
    "        end_date=\"20211231235959\",     # âœ… 2021ë…„ ì¢…ë£Œì¼\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2021_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")  # âœ… ì €ì¥ íŒŒì¼ëª…ë„ 2021ë¡œ\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2021_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c18ac-cc45-4f0f-bad0-65489bc569ea",
   "metadata": {
    "id": "f15c18ac-cc45-4f0f-bad0-65489bc569ea"
   },
   "source": [
    "í¬ë¡¤ë§ 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bedc91-9a00-4404-911b-350eb8f23c51",
   "metadata": {
    "id": "b1bedc91-9a00-4404-911b-350eb8f23c51"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"  # âœ… í´ëŸ¬ìŠ¤í„° ì˜µì…˜ ì¶”ê°€\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={p}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20220101000000\",   # âœ… 2022ë…„ ì‹œì‘ì¼\n",
    "    end_date=\"20221231235959\",     # âœ… 2022ë…„ ì¢…ë£Œì¼\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20220101000000\",   # âœ… 2022ë…„ ì‹œì‘ì¼\n",
    "        end_date=\"20221231235959\",     # âœ… 2022ë…„ ì¢…ë£Œì¼\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2022_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")  # âœ… ì €ì¥ íŒŒì¼ëª…ë„ 2022ë¡œ\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2022_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67b3d0-33c5-4843-b00b-a2c07f45dc41",
   "metadata": {
    "id": "ad67b3d0-33c5-4843-b00b-a2c07f45dc41"
   },
   "source": [
    "í¬ë¡¤ë§ 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993278cf-50ab-4c05-b4b1-ea79ba2e9453",
   "metadata": {
    "id": "993278cf-50ab-4c05-b4b1-ea79ba2e9453"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def collect_ids(keyword, start_date, end_date, max_pages=69):\n",
    "    opts = Options()\n",
    "    opts.add_argument('--headless')\n",
    "    opts.add_argument('--disable-gpu')\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "\n",
    "    kw = keyword.replace(\" \", \"+\")\n",
    "    base = (\n",
    "        \"https://search.daum.net/search\"\n",
    "        f\"?w=news&nil_search=btn&DA=STC&enc=utf8\"\n",
    "        f\"&cluster=y&cluster_page=1\"  # âœ… í´ëŸ¬ìŠ¤í„° ì˜µì…˜ í¬í•¨\n",
    "        f\"&q={kw}\"\n",
    "        f\"&sd={start_date}&ed={end_date}\"\n",
    "        f\"&period=u&sort=recency\"\n",
    "    )\n",
    "\n",
    "    seen, ids = set(), []\n",
    "    for p in range(1, max_pages + 1):\n",
    "        url = f\"{base}&p={p}\"\n",
    "        driver.get(url)\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        elems = driver.find_elements(By.CSS_SELECTOR, 'a[href*=\"//v.daum.net/v/\"]')\n",
    "        new = 0\n",
    "        for e in elems:\n",
    "            m = re.search(r'/v/(\\d+)', e.get_attribute('href'))\n",
    "            if m:\n",
    "                aid = m.group(1)\n",
    "                if aid not in seen:\n",
    "                    seen.add(aid)\n",
    "                    ids.append(aid)\n",
    "                    new += 1\n",
    "        print(f\"â–¶ p={p}: ì‹ ê·œ {new}ê±´, ëˆ„ì  {len(ids)}ê±´\")\n",
    "        if new == 0 and p > 5:\n",
    "            break\n",
    "\n",
    "    driver.quit()\n",
    "    return ids\n",
    "\n",
    "def parse_article(aid):\n",
    "    url = f\"https://v.daum.net/v/{aid}\"\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    r = requests.get(url, headers=headers, timeout=5)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    title = soup.select_one('meta[property=\"og:title\"]')['content'].strip()\n",
    "    press = soup.select_one('meta[property=\"og:article:author\"]')['content'].strip()\n",
    "\n",
    "    body = soup.select_one('div.article_view')\n",
    "    for t in body.select('script, iframe, ins, a, figure'):\n",
    "        t.decompose()\n",
    "    content = \"\\n\".join(p.get_text(strip=True) for p in body.select('p[dmcf-ptype=\"general\"]'))\n",
    "\n",
    "    meta = soup.select_one('meta[property=\"og:regDate\"]')\n",
    "    if meta and meta.get('content'):\n",
    "        year = int(meta['content'][:4])\n",
    "    else:\n",
    "        span = soup.select_one('span.num_date')\n",
    "        year = datetime.strptime(span.get_text(strip=True), '%Y. %m. %d. %H:%M').year\n",
    "\n",
    "    return title, content, press, year\n",
    "\n",
    "def crawl_daum_segment_fast(\n",
    "    keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "    start_date=\"20230101000000\",   # âœ… 2023ë…„ ì‹œì‘ì¼\n",
    "    end_date=\"20231231235959\",     # âœ… 2023ë…„ ì¢…ë£Œì¼\n",
    "    max_pages=69,\n",
    "    target_count=1000\n",
    "):\n",
    "    ids = collect_ids(keyword, start_date, end_date, max_pages)\n",
    "    ids = ids[:target_count]\n",
    "    print(f\"\\nğŸ”– íŒŒì‹± ëŒ€ìƒ ID: {len(ids)}ê±´\\n\")\n",
    "\n",
    "    records = []\n",
    "    for idx, aid in enumerate(ids, start=1):\n",
    "        try:\n",
    "            title, content, press, year = parse_article(aid)\n",
    "            records.append({\n",
    "                'ìˆœë²ˆ': idx,\n",
    "                'ì œëª©': title,\n",
    "                'ë³¸ë¬¸': content,\n",
    "                'ì‹ ë¬¸ì‚¬': press,\n",
    "                'ì—°ë„': year\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ [{aid}] íŒŒì‹± ì‹¤íŒ¨:\", e)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    df = pd.DataFrame(records, columns=['ìˆœë²ˆ', 'ì œëª©', 'ë³¸ë¬¸', 'ì‹ ë¬¸ì‚¬', 'ì—°ë„'])\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_daum_segment_fast(\n",
    "        keyword=\"ì •ì¹˜ì  ì˜¬ë°”ë¦„\",\n",
    "        start_date=\"20230101000000\",   # âœ… 2023ë…„ ì‹œì‘ì¼\n",
    "        end_date=\"20231231235959\",     # âœ… 2023ë…„ ì¢…ë£Œì¼\n",
    "        max_pages=69,\n",
    "        target_count=1000\n",
    "    )\n",
    "    print(df)\n",
    "    df.to_csv(\"daum_2023_pc_fast.csv\", index=False, encoding=\"utf-8-sig\")  # âœ… ì €ì¥ íŒŒì¼ëª…ë„ 2023ìœ¼ë¡œ\n",
    "    print(f\"âœ… ì´ {len(df)}ê±´ í¬ë¡¤ë§ ì™„ë£Œ â€” daum_2023_pc_fast.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decd6c21-e41c-42f5-bec4-ac0ff75e7cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
